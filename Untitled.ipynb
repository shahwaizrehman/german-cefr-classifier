{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ade3280f-b9a8-47a5-9280-acddadf0974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text CEFR\n",
      "0  M. Meier Müllergasse 1 Stadt X Internationale ...   B2\n",
      "1  Müller Julia Bahnhofsstr. 1 A Stadt X Armenien...   B2\n",
      "2  Michael Meier 1 Zentralplatz 1234. Stadt X Aup...   B2\n",
      "3  Eva Meier Schmidt Müllergasse 12 Stadt X Kroat...   B2\n",
      "4  Abs. Frau EVA SCHMIDT BAHNHOFSTR, , 1234 STADT...   B1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"cefr_augmented.csv\")\n",
    "\n",
    "# Check the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91092abb-9d59-4062-8243-39d6fa19f5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation done! Total rows: 1239\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "import time\n",
    "\n",
    "# Load your cleaned CSV\n",
    "df = pd.read_csv(\"merlin_meta_german_clean.csv\")\n",
    "\n",
    "# Only minority classes\n",
    "minority_df = df[df['CEFR'].isin(['A1', 'C1', 'C2'])]\n",
    "\n",
    "translator = Translator()\n",
    "augmented_texts = []\n",
    "\n",
    "# Number of augmented versions per text\n",
    "n_augment = 2  # You can change to 1,2,3\n",
    "\n",
    "for idx, row in minority_df.iterrows():\n",
    "    text = row['text']\n",
    "    label = row['CEFR']\n",
    "    \n",
    "    for i in range(n_augment):\n",
    "        try:\n",
    "            # German -> English\n",
    "            en_text = translator.translate(text, src='de', dest='en').text\n",
    "            # English -> German\n",
    "            de_text = translator.translate(en_text, src='en', dest='de').text\n",
    "            augmented_texts.append({'text': de_text, 'CEFR': label})\n",
    "            time.sleep(1)  # avoid hitting Google too fast\n",
    "        except Exception as e:\n",
    "            print(f\"Translation failed for index {idx}: {e}\")\n",
    "\n",
    "# Create DataFrame of augmented texts\n",
    "aug_df = pd.DataFrame(augmented_texts)\n",
    "\n",
    "# Combine with original data\n",
    "df_augmented = pd.concat([df, aug_df], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "df_augmented.to_csv(\"cefr_augmented.csv\", index=False, encoding='utf-8')\n",
    "print(\"Augmentation done! Total rows:\", len(df_augmented))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "6e586546-0456-44bb-b3c3-205a6b3cf152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [0.73267327 0.74752475 0.75124378 0.74129353 0.73134328]\n",
      "Mean CV Accuracy: 0.7408157233633811\n",
      "Test Set Accuracy: 0.6984126984126984\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.71      0.69      0.70        35\n",
      "          A2       0.63      0.75      0.68        63\n",
      "          B1       0.71      0.54      0.61        67\n",
      "          B2       0.68      0.83      0.75        59\n",
      "          C1       1.00      0.71      0.83        28\n",
      "\n",
      "    accuracy                           0.70       252\n",
      "   macro avg       0.74      0.70      0.71       252\n",
      "weighted avg       0.71      0.70      0.70       252\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[24 11  0  0  0]\n",
      " [ 9 47  6  1  0]\n",
      " [ 0 17 36 14  0]\n",
      " [ 1  0  9 49  0]\n",
      " [ 0  0  0  8 20]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cefr_augmented.csv\", encoding='latin1')\n",
    "df['text'] = df['text'].str.replace('Ã¼', 'ü')\n",
    "df['text'] = df['text'].str.replace('Ã¤', 'ä')\n",
    "df['text'] = df['text'].str.replace('Ã¶', 'ö')\n",
    "df['text'] = df['text'].str.replace('ÃŸ', 'ß')\n",
    "\n",
    "# Merge rare class (assuming C2 is sparse)\n",
    "df['CEFR'] = df['CEFR'].replace({'C2': 'C1'})\n",
    "\n",
    "# Features and labels\n",
    "X = df['text']\n",
    "y = df['CEFR']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# TF-IDF vectorizer with fixes for German\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 3),\n",
    "    sublinear_tf=True,\n",
    "    lowercase=False,  # Preserve case for German nouns\n",
    "    strip_accents=None  # Critical: Keep umlauts and accents intact\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Random Forest model (alternative to Logistic; tune n_estimators if needed)\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Cross-validation for better accuracy estimate\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_train_vec, y_train, cv=cv, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "# Fit on full train set\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred, labels=['A1', 'A2', 'B1', 'B2', 'C1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "8af86c9b-04e9-4726-a01f-fb858e313da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-12 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-12 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-12 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-12 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-12 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-12 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-12 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-12 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-12 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-12 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-12 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-12 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-12 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, n_estimators=200,\n",
       "                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, n_estimators=200,\n",
       "                       random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', n_estimators=200,\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=15000, ngram_range=(1,3))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "...\n",
    "model.fit(X_train_vec, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d47353b2-0c23-46a5-bbd8-2e01d53bc96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cefr(text):\n",
    "    # Transform the text using the previously fitted vectorizer\n",
    "    text_vec = vectorizer.transform([text])\n",
    "    \n",
    "    # Predict CEFR level\n",
    "    prediction = model.predict(text_vec)[0]\n",
    "    \n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "7f95634f-4fa6-44c0-9368-d064a0eeb589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted CEFR level: A1\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "Er hat sich stundenlang auf die Präsentation vorbereitet.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Predicted CEFR level:\", predict_cefr(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8427e84f-35ac-4c83-a347-b1c8836deaff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[291], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load data (same as above)\n",
    "df = pd.read_csv(\"cefr_augmented.csv\", encoding='latin1')\n",
    "# ... (cleaning and merging as before)\n",
    "\n",
    "# Prepare dataset\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['CEFR'])\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'CEFR']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['text', 'CEFR']])\n",
    "\n",
    "# Label mapping\n",
    "labels = sorted(df['CEFR'].unique())\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in id2label.items()}\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/gelectra-base\")\n",
    "\n",
    "def preprocess(examples):\n",
    "    examples['label'] = [label2id[l] for l in examples['CEFR']]\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"deepset/gelectra-base\", num_labels=len(labels), id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "preds = trainer.predict(test_dataset).predictions.argmax(-1)\n",
    "print(classification_report(test_dataset['label'], preds, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "66014084-a5cf-492c-b75b-ee6c6eeac16d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboostNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading xgboost-3.1.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.11-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: nltk in d:\\anaconda\\lib\\site-packages (from textstat) (3.9.1)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from textstat) (75.1.0)\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from nltk->textstat) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from nltk->textstat) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\lib\\site-packages (from nltk->textstat) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from nltk->textstat) (4.66.5)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from click->nltk->textstat) (0.4.6)\n",
      "Downloading xgboost-3.1.2-py3-none-win_amd64.whl (72.0 MB)\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/72.0 MB 2.8 MB/s eta 0:00:26\n",
      "   - -------------------------------------- 1.8/72.0 MB 5.3 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 2.4/72.0 MB 3.4 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 2.4/72.0 MB 3.4 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 3.1/72.0 MB 2.9 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 3.4/72.0 MB 2.5 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 3.7/72.0 MB 2.4 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 4.2/72.0 MB 2.5 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 4.7/72.0 MB 2.3 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 4.7/72.0 MB 2.3 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 5.0/72.0 MB 2.2 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 5.5/72.0 MB 2.0 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 5.8/72.0 MB 2.0 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 6.0/72.0 MB 2.0 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 6.3/72.0 MB 1.9 MB/s eta 0:00:34\n",
      "   --- ------------------------------------ 6.8/72.0 MB 2.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 7.3/72.0 MB 2.0 MB/s eta 0:00:33\n",
      "   ---- ----------------------------------- 7.6/72.0 MB 2.0 MB/s eta 0:00:33\n",
      "   ---- ----------------------------------- 8.1/72.0 MB 1.9 MB/s eta 0:00:33\n",
      "   ---- ----------------------------------- 8.4/72.0 MB 1.9 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 8.7/72.0 MB 1.9 MB/s eta 0:00:34\n",
      "   ----- ---------------------------------- 9.2/72.0 MB 1.9 MB/s eta 0:00:34\n",
      "   ----- ---------------------------------- 9.4/72.0 MB 1.8 MB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 9.7/72.0 MB 1.8 MB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 10.2/72.0 MB 1.8 MB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 10.5/72.0 MB 1.8 MB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 10.7/72.0 MB 1.8 MB/s eta 0:00:35\n",
      "   ------ --------------------------------- 11.3/72.0 MB 1.8 MB/s eta 0:00:35\n",
      "   ------ --------------------------------- 11.5/72.0 MB 1.8 MB/s eta 0:00:35\n",
      "   ------ --------------------------------- 11.8/72.0 MB 1.8 MB/s eta 0:00:35\n",
      "   ------ --------------------------------- 12.3/72.0 MB 1.8 MB/s eta 0:00:35\n",
      "   ------ --------------------------------- 12.6/72.0 MB 1.8 MB/s eta 0:00:34\n",
      "   ------- -------------------------------- 12.8/72.0 MB 1.7 MB/s eta 0:00:34\n",
      "   ------- -------------------------------- 13.4/72.0 MB 1.7 MB/s eta 0:00:34\n",
      "   ------- -------------------------------- 13.6/72.0 MB 1.7 MB/s eta 0:00:34\n",
      "   ------- -------------------------------- 13.9/72.0 MB 1.7 MB/s eta 0:00:34\n",
      "   ------- -------------------------------- 14.2/72.0 MB 1.7 MB/s eta 0:00:34\n",
      "   -------- ------------------------------- 14.7/72.0 MB 1.7 MB/s eta 0:00:34\n",
      "   -------- ------------------------------- 14.9/72.0 MB 1.7 MB/s eta 0:00:34\n",
      "   -------- ------------------------------- 15.5/72.0 MB 1.7 MB/s eta 0:00:33\n",
      "   -------- ------------------------------- 15.7/72.0 MB 1.7 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 16.3/72.0 MB 1.7 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 16.5/72.0 MB 1.7 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 17.0/72.0 MB 1.7 MB/s eta 0:00:32\n",
      "   --------- ------------------------------ 17.3/72.0 MB 1.7 MB/s eta 0:00:32\n",
      "   --------- ------------------------------ 17.8/72.0 MB 1.7 MB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 18.1/72.0 MB 1.7 MB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 18.6/72.0 MB 1.7 MB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 18.9/72.0 MB 1.7 MB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 19.1/72.0 MB 1.7 MB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 19.7/72.0 MB 1.7 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 19.9/72.0 MB 1.7 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 19.9/72.0 MB 1.7 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 20.2/72.0 MB 1.7 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 20.2/72.0 MB 1.7 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 20.4/72.0 MB 1.7 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 20.4/72.0 MB 1.7 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 20.7/72.0 MB 1.6 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 20.7/72.0 MB 1.6 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 21.0/72.0 MB 1.6 MB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 21.0/72.0 MB 1.6 MB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 21.2/72.0 MB 1.6 MB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 21.2/72.0 MB 1.6 MB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 21.5/72.0 MB 1.5 MB/s eta 0:00:34\n",
      "   ----------- ---------------------------- 21.5/72.0 MB 1.5 MB/s eta 0:00:34\n",
      "   ------------ --------------------------- 21.8/72.0 MB 1.5 MB/s eta 0:00:34\n",
      "   ------------ --------------------------- 22.0/72.0 MB 1.5 MB/s eta 0:00:34\n",
      "   ------------ --------------------------- 22.0/72.0 MB 1.5 MB/s eta 0:00:34\n",
      "   ------------ --------------------------- 22.3/72.0 MB 1.5 MB/s eta 0:00:34\n",
      "   ------------ --------------------------- 22.5/72.0 MB 1.5 MB/s eta 0:00:34\n",
      "   ------------ --------------------------- 22.5/72.0 MB 1.5 MB/s eta 0:00:34\n",
      "   ------------ --------------------------- 22.8/72.0 MB 1.4 MB/s eta 0:00:35\n",
      "   ------------ --------------------------- 23.1/72.0 MB 1.4 MB/s eta 0:00:35\n",
      "   ------------ --------------------------- 23.3/72.0 MB 1.4 MB/s eta 0:00:35\n",
      "   ------------ --------------------------- 23.3/72.0 MB 1.4 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 23.6/72.0 MB 1.4 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 23.9/72.0 MB 1.4 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 24.1/72.0 MB 1.4 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 24.4/72.0 MB 1.4 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 24.4/72.0 MB 1.4 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 24.6/72.0 MB 1.4 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 24.9/72.0 MB 1.4 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 25.2/72.0 MB 1.4 MB/s eta 0:00:34\n",
      "   -------------- ------------------------- 25.4/72.0 MB 1.4 MB/s eta 0:00:34\n",
      "   -------------- ------------------------- 25.7/72.0 MB 1.4 MB/s eta 0:00:34\n",
      "   -------------- ------------------------- 26.0/72.0 MB 1.4 MB/s eta 0:00:34\n",
      "   -------------- ------------------------- 26.2/72.0 MB 1.4 MB/s eta 0:00:34\n",
      "   -------------- ------------------------- 26.5/72.0 MB 1.4 MB/s eta 0:00:34\n",
      "   -------------- ------------------------- 26.7/72.0 MB 1.4 MB/s eta 0:00:34\n",
      "   --------------- ------------------------ 27.0/72.0 MB 1.4 MB/s eta 0:00:33\n",
      "   --------------- ------------------------ 27.3/72.0 MB 1.4 MB/s eta 0:00:33\n",
      "   --------------- ------------------------ 27.8/72.0 MB 1.4 MB/s eta 0:00:33\n",
      "   --------------- ------------------------ 28.0/72.0 MB 1.4 MB/s eta 0:00:33\n",
      "   --------------- ------------------------ 28.6/72.0 MB 1.4 MB/s eta 0:00:32\n",
      "   ---------------- ----------------------- 28.8/72.0 MB 1.4 MB/s eta 0:00:32\n",
      "   ---------------- ----------------------- 29.4/72.0 MB 1.4 MB/s eta 0:00:31\n",
      "   ---------------- ----------------------- 29.9/72.0 MB 1.4 MB/s eta 0:00:31\n",
      "   ---------------- ----------------------- 30.4/72.0 MB 1.4 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 31.2/72.0 MB 1.4 MB/s eta 0:00:29\n",
      "   ----------------- ---------------------- 31.5/72.0 MB 1.4 MB/s eta 0:00:29\n",
      "   ----------------- ---------------------- 32.2/72.0 MB 1.4 MB/s eta 0:00:28\n",
      "   ------------------ --------------------- 32.5/72.0 MB 1.4 MB/s eta 0:00:28\n",
      "   ------------------ --------------------- 32.8/72.0 MB 1.5 MB/s eta 0:00:28\n",
      "   ------------------ --------------------- 33.3/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------ --------------------- 33.6/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------ --------------------- 33.8/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------ --------------------- 34.1/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 34.3/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 34.3/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 34.6/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 34.6/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 34.9/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 34.9/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 34.9/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 35.1/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 35.1/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 35.1/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 35.1/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 35.4/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 35.4/72.0 MB 1.4 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 35.7/72.0 MB 1.3 MB/s eta 0:00:28\n",
      "   ------------------- -------------------- 35.7/72.0 MB 1.3 MB/s eta 0:00:28\n",
      "   ------------------- -------------------- 35.7/72.0 MB 1.3 MB/s eta 0:00:28\n",
      "   ------------------- -------------------- 35.9/72.0 MB 1.3 MB/s eta 0:00:28\n",
      "   ------------------- -------------------- 35.9/72.0 MB 1.3 MB/s eta 0:00:28\n",
      "   ------------------- -------------------- 35.9/72.0 MB 1.3 MB/s eta 0:00:28\n",
      "   -------------------- ------------------- 36.2/72.0 MB 1.3 MB/s eta 0:00:28\n",
      "   -------------------- ------------------- 36.2/72.0 MB 1.3 MB/s eta 0:00:28\n",
      "   -------------------- ------------------- 36.4/72.0 MB 1.3 MB/s eta 0:00:28\n",
      "   -------------------- ------------------- 36.4/72.0 MB 1.3 MB/s eta 0:00:28\n",
      "   -------------------- ------------------- 36.4/72.0 MB 1.3 MB/s eta 0:00:28\n",
      "   -------------------- ------------------- 36.4/72.0 MB 1.3 MB/s eta 0:00:28\n",
      "   -------------------- ------------------- 36.7/72.0 MB 1.3 MB/s eta 0:00:29\n",
      "   -------------------- ------------------- 36.7/72.0 MB 1.3 MB/s eta 0:00:29\n",
      "   -------------------- ------------------- 36.7/72.0 MB 1.3 MB/s eta 0:00:29\n",
      "   -------------------- ------------------- 37.0/72.0 MB 1.2 MB/s eta 0:00:29\n",
      "   -------------------- ------------------- 37.0/72.0 MB 1.2 MB/s eta 0:00:29\n",
      "   -------------------- ------------------- 37.2/72.0 MB 1.2 MB/s eta 0:00:29\n",
      "   -------------------- ------------------- 37.2/72.0 MB 1.2 MB/s eta 0:00:29\n",
      "   -------------------- ------------------- 37.2/72.0 MB 1.2 MB/s eta 0:00:29\n",
      "   -------------------- ------------------- 37.5/72.0 MB 1.2 MB/s eta 0:00:30\n",
      "   -------------------- ------------------- 37.5/72.0 MB 1.2 MB/s eta 0:00:30\n",
      "   -------------------- ------------------- 37.5/72.0 MB 1.2 MB/s eta 0:00:30\n",
      "   -------------------- ------------------- 37.7/72.0 MB 1.1 MB/s eta 0:00:31\n",
      "   -------------------- ------------------- 37.7/72.0 MB 1.1 MB/s eta 0:00:31\n",
      "   -------------------- ------------------- 37.7/72.0 MB 1.1 MB/s eta 0:00:31\n",
      "   --------------------- ------------------ 38.0/72.0 MB 1.1 MB/s eta 0:00:31\n",
      "   --------------------- ------------------ 38.0/72.0 MB 1.1 MB/s eta 0:00:31\n",
      "   --------------------- ------------------ 38.3/72.0 MB 1.1 MB/s eta 0:00:31\n",
      "   --------------------- ------------------ 38.3/72.0 MB 1.1 MB/s eta 0:00:31\n",
      "   --------------------- ------------------ 38.5/72.0 MB 1.1 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 38.5/72.0 MB 1.1 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 38.8/72.0 MB 1.1 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 38.8/72.0 MB 1.1 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 39.1/72.0 MB 1.0 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 39.1/72.0 MB 1.0 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 39.3/72.0 MB 1.0 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 39.3/72.0 MB 1.0 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 39.6/72.0 MB 1.0 MB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 39.8/72.0 MB 1.0 MB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 39.8/72.0 MB 1.0 MB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 40.1/72.0 MB 1.0 MB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 40.4/72.0 MB 996.7 kB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 40.4/72.0 MB 996.7 kB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 40.6/72.0 MB 988.4 kB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 40.9/72.0 MB 980.2 kB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 40.9/72.0 MB 980.2 kB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 41.2/72.0 MB 973.0 kB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 41.4/72.0 MB 970.5 kB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 41.7/72.0 MB 962.7 kB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 41.9/72.0 MB 956.4 kB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 42.2/72.0 MB 953.4 kB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 42.2/72.0 MB 953.4 kB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 42.5/72.0 MB 947.2 kB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 42.7/72.0 MB 945.7 kB/s eta 0:00:31\n",
      "   ------------------------ --------------- 43.3/72.0 MB 938.4 kB/s eta 0:00:31\n",
      "   ------------------------ --------------- 43.5/72.0 MB 937.9 kB/s eta 0:00:31\n",
      "   ------------------------ --------------- 43.8/72.0 MB 936.9 kB/s eta 0:00:31\n",
      "   ------------------------ --------------- 44.0/72.0 MB 936.4 kB/s eta 0:00:30\n",
      "   ------------------------ --------------- 44.6/72.0 MB 935.9 kB/s eta 0:00:30\n",
      "   ------------------------- -------------- 45.1/72.0 MB 937.4 kB/s eta 0:00:29\n",
      "   ------------------------- -------------- 45.4/72.0 MB 937.9 kB/s eta 0:00:29\n",
      "   ------------------------- -------------- 45.9/72.0 MB 943.7 kB/s eta 0:00:28\n",
      "   ------------------------- -------------- 46.4/72.0 MB 945.7 kB/s eta 0:00:28\n",
      "   -------------------------- ------------- 46.9/72.0 MB 952.5 kB/s eta 0:00:27\n",
      "   -------------------------- ------------- 47.7/72.0 MB 961.7 kB/s eta 0:00:26\n",
      "   -------------------------- ------------- 48.2/72.0 MB 971.5 kB/s eta 0:00:25\n",
      "   --------------------------- ------------ 48.8/72.0 MB 971.9 kB/s eta 0:00:24\n",
      "   --------------------------- ------------ 49.3/72.0 MB 980.2 kB/s eta 0:00:24\n",
      "   --------------------------- ------------ 49.5/72.0 MB 986.9 kB/s eta 0:00:23\n",
      "   --------------------------- ------------ 49.5/72.0 MB 986.9 kB/s eta 0:00:23\n",
      "   --------------------------- ------------ 49.8/72.0 MB 983.8 kB/s eta 0:00:23\n",
      "   --------------------------- ------------ 49.8/72.0 MB 983.8 kB/s eta 0:00:23\n",
      "   --------------------------- ------------ 50.1/72.0 MB 986.4 kB/s eta 0:00:23\n",
      "   --------------------------- ------------ 50.3/72.0 MB 991.1 kB/s eta 0:00:22\n",
      "   --------------------------- ------------ 50.3/72.0 MB 991.1 kB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 50.6/72.0 MB 990.5 kB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 50.9/72.0 MB 995.2 kB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 51.1/72.0 MB 999.4 kB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 51.4/72.0 MB 998.7 kB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 51.4/72.0 MB 998.7 kB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 51.6/72.0 MB 1.0 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 51.9/72.0 MB 1.0 MB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 52.2/72.0 MB 1.0 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 52.4/72.0 MB 1.0 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 52.7/72.0 MB 1.0 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 53.0/72.0 MB 1.0 MB/s eta 0:00:19\n",
      "   ----------------------------- ---------- 53.2/72.0 MB 1.0 MB/s eta 0:00:19\n",
      "   ----------------------------- ---------- 53.2/72.0 MB 1.0 MB/s eta 0:00:19\n",
      "   ----------------------------- ---------- 53.7/72.0 MB 1.0 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 54.0/72.0 MB 1.0 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 54.3/72.0 MB 1.0 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 54.5/72.0 MB 1.0 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 54.8/72.0 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------------------------ --------- 55.1/72.0 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------------------------ --------- 55.3/72.0 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------------------------ --------- 55.6/72.0 MB 1.0 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 55.8/72.0 MB 1.0 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 56.1/72.0 MB 1.0 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 56.4/72.0 MB 1.0 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 56.6/72.0 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------------- -------- 57.1/72.0 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------------- -------- 57.4/72.0 MB 1.0 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 57.7/72.0 MB 1.0 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 57.9/72.0 MB 1.0 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 58.5/72.0 MB 1.0 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 58.7/72.0 MB 1.1 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 59.0/72.0 MB 1.1 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 59.5/72.0 MB 1.1 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 59.8/72.0 MB 1.1 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 60.3/72.0 MB 1.1 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 60.8/72.0 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 61.3/72.0 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 61.9/72.0 MB 1.1 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 62.4/72.0 MB 1.1 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 62.7/72.0 MB 1.1 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 62.9/72.0 MB 1.0 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 63.4/72.0 MB 1.0 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 63.7/72.0 MB 1.0 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 64.0/72.0 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 64.0/72.0 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 64.2/72.0 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 64.5/72.0 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 64.7/72.0 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 65.0/72.0 MB 1.0 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 65.3/72.0 MB 1.0 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 65.5/72.0 MB 1.0 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 65.8/72.0 MB 1.0 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 66.1/72.0 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 66.3/72.0 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 66.6/72.0 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 66.8/72.0 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 67.1/72.0 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 67.4/72.0 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 67.6/72.0 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 67.9/72.0 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 68.2/72.0 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 68.4/72.0 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 68.9/72.0 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 69.2/72.0 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 69.5/72.0 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 69.7/72.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  70.3/72.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  70.5/72.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  70.8/72.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------------  71.0/72.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.6/72.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.0/72.0 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading textstat-0.7.11-py3-none-any.whl (176 kB)\n",
      "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 164.5 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 0.8/2.1 MB 51.5 kB/s eta 0:00:26\n",
      "   -------------------- ------------------- 1.0/2.1 MB 47.3 kB/s eta 0:00:22\n",
      "   -------------------- ------------------- 1.0/2.1 MB 47.3 kB/s eta 0:00:22\n",
      "   -------------------- ------------------- 1.0/2.1 MB 47.3 kB/s eta 0:00:22\n",
      "   -------------------- ------------------- 1.0/2.1 MB 47.3 kB/s eta 0:00:22\n",
      "   -------------------- ------------------- 1.0/2.1 MB 47.3 kB/s eta 0:00:22\n",
      "   -------------------- ------------------- 1.0/2.1 MB 47.3 kB/s eta 0:00:22\n",
      "   -------------------- ------------------- 1.0/2.1 MB 47.3 kB/s eta 0:00:22\n",
      "   ------------------------- -------------- 1.3/2.1 MB 57.3 kB/s eta 0:00:14\n",
      "   ------------------------- -------------- 1.3/2.1 MB 57.3 kB/s eta 0:00:14\n",
      "   ------------------------- -------------- 1.3/2.1 MB 57.3 kB/s eta 0:00:14\n",
      "   ------------------------- -------------- 1.3/2.1 MB 57.3 kB/s eta 0:00:14\n",
      "   ------------------------- -------------- 1.3/2.1 MB 57.3 kB/s eta 0:00:14\n",
      "   ------------------------- -------------- 1.3/2.1 MB 57.3 kB/s eta 0:00:14\n",
      "   ------------------------- -------------- 1.3/2.1 MB 57.3 kB/s eta 0:00:14\n",
      "   ------------------------------ --------- 1.6/2.1 MB 66.1 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 1.6/2.1 MB 66.1 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 1.6/2.1 MB 66.1 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 1.6/2.1 MB 66.1 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 1.6/2.1 MB 66.1 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 1.6/2.1 MB 66.1 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 1.6/2.1 MB 66.1 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 1.6/2.1 MB 66.1 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 1.6/2.1 MB 66.1 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 1.6/2.1 MB 66.1 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 1.6/2.1 MB 66.1 kB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 70.7 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 70.7 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 70.7 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 70.7 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 70.7 kB/s eta 0:00:04\n",
      "   ---------------------------------------- 2.1/2.1 MB 77.9 kB/s eta 0:00:00\n",
      "Installing collected packages: pyphen, xgboost, textstat\n",
      "Successfully installed pyphen-0.17.2 textstat-0.7.11 xgboost-3.1.2\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "72e2fbe9-d3b6-4b3a-9853-d641633db89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [18:58:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [18:59:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [18:59:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [19:00:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [19:01:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [0.7029703  0.72772277 0.68656716 0.70646766 0.70149254]\n",
      "Mean CV Accuracy: 0.705044086498202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [19:02:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.7341269841269841\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.82      0.77      0.79        35\n",
      "          A2       0.76      0.76      0.76        63\n",
      "          B1       0.67      0.72      0.69        67\n",
      "          B2       0.67      0.75      0.70        59\n",
      "          C1       1.00      0.64      0.78        28\n",
      "\n",
      "    accuracy                           0.73       252\n",
      "   macro avg       0.78      0.73      0.75       252\n",
      "weighted avg       0.75      0.73      0.74       252\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[27  8  0  0  0]\n",
      " [ 5 48 10  0  0]\n",
      " [ 0  7 48 12  0]\n",
      " [ 1  0 14 44  0]\n",
      " [ 0  0  0 10 18]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import textstat  # For readability scores\n",
    "\n",
    "# Custom transformer for linguistic features\n",
    "class LinguisticFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for text in X:\n",
    "            words = text.split()\n",
    "            num_words = len(words)\n",
    "            num_unique_words = len(set(words))\n",
    "            avg_word_len = np.mean([len(w) for w in words]) if num_words > 0 else 0\n",
    "            ttr = num_unique_words / num_words if num_words > 0 else 0\n",
    "            sent_len = len(text)  # Char length as proxy for sentence complexity\n",
    "            flesch = textstat.flesch_reading_ease(text)  # Adaptable to German\n",
    "            features.append([num_words, num_unique_words, avg_word_len, ttr, sent_len, flesch])\n",
    "        return np.array(features)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cefr_augmented.csv\", encoding='latin1')\n",
    "df['text'] = df['text'].str.replace('Ã¼', 'ü')\n",
    "df['text'] = df['text'].str.replace('Ã¤', 'ä')\n",
    "df['text'] = df['text'].str.replace('Ã¶', 'ö')\n",
    "df['text'] = df['text'].str.replace('ÃŸ', 'ß')\n",
    "\n",
    "# Merge rare class\n",
    "df['CEFR'] = df['CEFR'].replace({'C2': 'C1'})\n",
    "\n",
    "# Features and labels\n",
    "X = df[['text']]  # Use DF for column transformer\n",
    "y = df['CEFR']\n",
    "\n",
    "# Encode labels for XGBoost (needs numeric)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Pipeline: TF-IDF on text + linguistic features\n",
    "preprocessor = make_column_transformer(\n",
    "    (TfidfVectorizer(max_features=15000, ngram_range=(1, 3), sublinear_tf=True, lowercase=False, strip_accents=None), 'text'),\n",
    "    (LinguisticFeatures(), 'text')\n",
    ")\n",
    "\n",
    "# XGBoost model\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(n_estimators=200, random_state=42, eval_metric='mlogloss', scale_pos_weight=1))  # Handles multi-class\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "# Fit on full train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "y_test_labels = le.inverse_transform(y_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Test Set Accuracy:\", accuracy_score(y_test_labels, y_pred_labels))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_labels, y_pred_labels))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_labels, y_pred_labels, labels=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "b3157de0-2912-4ff6-b33b-bd3f34ee6082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in d:\\anaconda\\lib\\site-packages (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\anaconda\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\anaconda\\lib\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in d:\\anaconda\\lib\\site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\anaconda\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "c61d5416-ae78-4822-b279-ab020cbfc83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 6, 'classifier__n_estimators': 200}\n",
      "Cross-Validation Accuracy Scores: [0.72277228 0.73762376 0.71144279 0.72139303 0.71144279]\n",
      "Mean CV Accuracy: 0.7209349293138269\n",
      "Test Set Accuracy: 0.7261904761904762\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.79      0.74      0.76        35\n",
      "          A2       0.74      0.76      0.75        63\n",
      "          B1       0.67      0.69      0.68        67\n",
      "          B2       0.67      0.76      0.71        59\n",
      "          C1       1.00      0.64      0.78        28\n",
      "\n",
      "    accuracy                           0.73       252\n",
      "   macro avg       0.77      0.72      0.74       252\n",
      "weighted avg       0.74      0.73      0.73       252\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[26  9  0  0  0]\n",
      " [ 5 48 10  0  0]\n",
      " [ 1  8 46 12  0]\n",
      " [ 1  0 13 45  0]\n",
      " [ 0  0  0 10 18]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # For SMOTE integration\n",
    "import textstat\n",
    "\n",
    "# Custom transformer for linguistic features (enhanced with syllable proxy)\n",
    "class LinguisticFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for text in X:\n",
    "            words = [w.strip('.,!?;:\"()') for w in text.split() if w.isalpha()]  # Clean words\n",
    "            num_words = len(words)\n",
    "            num_unique_words = len(set(words))\n",
    "            avg_word_len = np.mean([len(w) for w in words]) if num_words > 0 else 0\n",
    "            ttr = num_unique_words / num_words if num_words > 0 else 0\n",
    "            sent_len = len(text)\n",
    "            flesch = textstat.flesch_reading_ease(text)\n",
    "            \n",
    "            # Simple German syllable proxy: count vowels + adjustments (ä/ö/ü/ei count extra)\n",
    "            def syllable_count(word):\n",
    "                vowels = 'aeiouäöüAEIOUÄÖÜ'\n",
    "                count = sum(1 for char in word if char in vowels)\n",
    "                if any(dip in word.lower() for dip in ['ei', 'ai', 'au', 'eu', 'äu']):\n",
    "                    count += 1  # Diphthongs\n",
    "                return max(1, count)\n",
    "            \n",
    "            avg_syllables = np.mean([syllable_count(w) for w in words]) if num_words > 0 else 0\n",
    "            features.append([num_words, num_unique_words, avg_word_len, ttr, sent_len, flesch, avg_syllables])\n",
    "        return np.array(features)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cefr_augmented.csv\", encoding='latin1')\n",
    "df['text'] = df['text'].str.replace('Ã¼', 'ü')\n",
    "df['text'] = df['text'].str.replace('Ã¤', 'ä')\n",
    "df['text'] = df['text'].str.replace('Ã¶', 'ö')\n",
    "df['text'] = df['text'].str.replace('ÃŸ', 'ß')\n",
    "\n",
    "# Merge rare class\n",
    "df['CEFR'] = df['CEFR'].replace({'C2': 'C1'})\n",
    "\n",
    "# Ordinal encoding for ordered loss (A1=0, A2=1, B1=2, B2=3, C1=4)\n",
    "ordinal_map = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4}\n",
    "df['CEFR_ordinal'] = df['CEFR'].map(ordinal_map)\n",
    "y = df['CEFR']\n",
    "y_ordinal = df['CEFR_ordinal']\n",
    "\n",
    "# Features\n",
    "X = df[['text']]\n",
    "\n",
    "# Encode categorical y for reports\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Train-test split (use ordinal for training, categorical for eval)\n",
    "X_train, X_test, y_train_ord, y_test_ord = train_test_split(\n",
    "    X, y_ordinal, test_size=0.2, random_state=42, stratify=y_ordinal\n",
    ")\n",
    "y_train_cat = le.transform([le.classes_[i] for i in y_train_ord])\n",
    "y_test_cat = le.transform([le.classes_[i] for i in y_test_ord])\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = make_column_transformer(\n",
    "    (TfidfVectorizer(max_features=15000, ngram_range=(1, 3), sublinear_tf=True, lowercase=False, strip_accents=None), 'text'),\n",
    "    (LinguisticFeatures(), 'text'),\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Pipeline with SMOTE + XGBoost (treat as multi-class on ordinal labels)\n",
    "model = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42, k_neighbors=3)),  # Oversample minorities\n",
    "    ('classifier', XGBClassifier(random_state=42, eval_metric='mlogloss', objective='multi:softprob'))\n",
    "])\n",
    "\n",
    "# Basic tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [150, 200],\n",
    "    'classifier__max_depth': [4, 6],\n",
    "    'classifier__learning_rate': [0.05, 0.1]\n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train_ord)\n",
    "\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "# Cross-validation on tuned model\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_train, y_train_ord, cv=cv, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "# Fit and predict (threshold ordinal preds to nearest class)\n",
    "y_pred_ord = model.predict(X_test)\n",
    "y_pred_cat = le.transform([le.classes_[np.round(p).astype(int)] for p in y_pred_ord])  # Round to nearest level\n",
    "\n",
    "# Evaluation\n",
    "print(\"Test Set Accuracy:\", accuracy_score(y_test_cat, y_pred_cat))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_cat, y_pred_cat, target_names=le.classes_))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_cat, y_pred_cat, labels=range(len(le.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "18eb34ba-99e2-4d13-80c9-334f03c1644e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in d:\\anaconda\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\anaconda\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in d:\\anaconda\\lib\\site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\anaconda\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in d:\\anaconda\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in d:\\anaconda\\lib\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\anaconda\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: hstspreload in d:\\anaconda\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.1.1)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda\\lib\\site-packages (from httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: chardet==3.* in d:\\anaconda\\lib\\site-packages (from httpx<1.0.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in d:\\anaconda\\lib\\site-packages (from httpx<1.0.0->datasets) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in d:\\anaconda\\lib\\site-packages (from httpx<1.0.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in d:\\anaconda\\lib\\site-packages (from httpx<1.0.0->datasets) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in d:\\anaconda\\lib\\site-packages (from httpcore==0.9.*->httpx<1.0.0->datasets) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in d:\\anaconda\\lib\\site-packages (from httpcore==0.9.*->httpx<1.0.0->datasets) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in d:\\anaconda\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx<1.0.0->datasets) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in d:\\anaconda\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx<1.0.0->datasets) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-win_amd64.whl (28.0 MB)\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/28.0 MB 2.1 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 1.0/28.0 MB 2.5 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 1.0/28.0 MB 2.5 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 1.6/28.0 MB 1.8 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 1.8/28.0 MB 1.8 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 2.1/28.0 MB 1.7 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 2.6/28.0 MB 1.6 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 2.6/28.0 MB 1.6 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 2.9/28.0 MB 1.5 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 3.1/28.0 MB 1.5 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 3.1/28.0 MB 1.5 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 3.4/28.0 MB 1.3 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 3.4/28.0 MB 1.3 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 3.7/28.0 MB 1.2 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 3.7/28.0 MB 1.2 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 3.9/28.0 MB 1.1 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 3.9/28.0 MB 1.1 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 4.2/28.0 MB 1.0 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 4.2/28.0 MB 1.0 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 4.5/28.0 MB 976.1 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 4.5/28.0 MB 976.1 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 4.7/28.0 MB 935.0 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 4.7/28.0 MB 935.0 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 4.7/28.0 MB 935.0 kB/s eta 0:00:25\n",
      "   ------- -------------------------------- 5.0/28.0 MB 890.8 kB/s eta 0:00:26\n",
      "   ------- -------------------------------- 5.0/28.0 MB 890.8 kB/s eta 0:00:26\n",
      "   ------- -------------------------------- 5.2/28.0 MB 841.1 kB/s eta 0:00:28\n",
      "   ------- -------------------------------- 5.2/28.0 MB 841.1 kB/s eta 0:00:28\n",
      "   ------- -------------------------------- 5.2/28.0 MB 841.1 kB/s eta 0:00:28\n",
      "   ------- -------------------------------- 5.5/28.0 MB 804.6 kB/s eta 0:00:28\n",
      "   ------- -------------------------------- 5.5/28.0 MB 804.6 kB/s eta 0:00:28\n",
      "   ------- -------------------------------- 5.5/28.0 MB 804.6 kB/s eta 0:00:28\n",
      "   -------- ------------------------------- 5.8/28.0 MB 781.2 kB/s eta 0:00:29\n",
      "   -------- ------------------------------- 5.8/28.0 MB 781.2 kB/s eta 0:00:29\n",
      "   -------- ------------------------------- 6.0/28.0 MB 764.2 kB/s eta 0:00:29\n",
      "   -------- ------------------------------- 6.0/28.0 MB 764.2 kB/s eta 0:00:29\n",
      "   -------- ------------------------------- 6.0/28.0 MB 764.2 kB/s eta 0:00:29\n",
      "   -------- ------------------------------- 6.3/28.0 MB 739.2 kB/s eta 0:00:30\n",
      "   -------- ------------------------------- 6.3/28.0 MB 739.2 kB/s eta 0:00:30\n",
      "   -------- ------------------------------- 6.3/28.0 MB 739.2 kB/s eta 0:00:30\n",
      "   --------- ------------------------------ 6.6/28.0 MB 713.9 kB/s eta 0:00:31\n",
      "   --------- ------------------------------ 6.6/28.0 MB 713.9 kB/s eta 0:00:31\n",
      "   --------- ------------------------------ 6.6/28.0 MB 713.9 kB/s eta 0:00:31\n",
      "   --------- ------------------------------ 6.8/28.0 MB 691.0 kB/s eta 0:00:31\n",
      "   --------- ------------------------------ 6.8/28.0 MB 691.0 kB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 7.1/28.0 MB 678.4 kB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 7.1/28.0 MB 678.4 kB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 7.1/28.0 MB 678.4 kB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 7.3/28.0 MB 671.1 kB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 7.3/28.0 MB 671.1 kB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 7.3/28.0 MB 671.1 kB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 7.6/28.0 MB 655.2 kB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 7.6/28.0 MB 655.2 kB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 7.6/28.0 MB 655.2 kB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 7.6/28.0 MB 655.2 kB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 7.6/28.0 MB 655.2 kB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 7.9/28.0 MB 617.4 kB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 7.9/28.0 MB 617.4 kB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 7.9/28.0 MB 617.4 kB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 7.9/28.0 MB 617.4 kB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 7.9/28.0 MB 617.4 kB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 7.9/28.0 MB 617.4 kB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 8.1/28.0 MB 579.2 kB/s eta 0:00:35\n",
      "   ----------- ---------------------------- 8.1/28.0 MB 579.2 kB/s eta 0:00:35\n",
      "   ----------- ---------------------------- 8.1/28.0 MB 579.2 kB/s eta 0:00:35\n",
      "   ----------- ---------------------------- 8.1/28.0 MB 579.2 kB/s eta 0:00:35\n",
      "   ----------- ---------------------------- 8.1/28.0 MB 579.2 kB/s eta 0:00:35\n",
      "   ----------- ---------------------------- 8.4/28.0 MB 555.7 kB/s eta 0:00:36\n",
      "   ----------- ---------------------------- 8.4/28.0 MB 555.7 kB/s eta 0:00:36\n",
      "   ----------- ---------------------------- 8.4/28.0 MB 555.7 kB/s eta 0:00:36\n",
      "   ------------ --------------------------- 8.7/28.0 MB 544.5 kB/s eta 0:00:36\n",
      "   ------------ --------------------------- 8.7/28.0 MB 544.5 kB/s eta 0:00:36\n",
      "   ------------ --------------------------- 8.7/28.0 MB 544.5 kB/s eta 0:00:36\n",
      "   ------------ --------------------------- 8.9/28.0 MB 539.1 kB/s eta 0:00:36\n",
      "   ------------ --------------------------- 8.9/28.0 MB 539.1 kB/s eta 0:00:36\n",
      "   ------------ --------------------------- 8.9/28.0 MB 539.1 kB/s eta 0:00:36\n",
      "   ------------- -------------------------- 9.2/28.0 MB 536.1 kB/s eta 0:00:36\n",
      "   ------------- -------------------------- 9.2/28.0 MB 536.1 kB/s eta 0:00:36\n",
      "   ------------- -------------------------- 9.4/28.0 MB 533.3 kB/s eta 0:00:35\n",
      "   ------------- -------------------------- 9.4/28.0 MB 533.3 kB/s eta 0:00:35\n",
      "   ------------- -------------------------- 9.4/28.0 MB 533.3 kB/s eta 0:00:35\n",
      "   ------------- -------------------------- 9.7/28.0 MB 529.8 kB/s eta 0:00:35\n",
      "   ------------- -------------------------- 9.7/28.0 MB 529.8 kB/s eta 0:00:35\n",
      "   ------------- -------------------------- 9.7/28.0 MB 529.8 kB/s eta 0:00:35\n",
      "   -------------- ------------------------- 10.0/28.0 MB 523.0 kB/s eta 0:00:35\n",
      "   -------------- ------------------------- 10.0/28.0 MB 523.0 kB/s eta 0:00:35\n",
      "   -------------- ------------------------- 10.0/28.0 MB 523.0 kB/s eta 0:00:35\n",
      "   -------------- ------------------------- 10.2/28.0 MB 518.3 kB/s eta 0:00:35\n",
      "   -------------- ------------------------- 10.2/28.0 MB 518.3 kB/s eta 0:00:35\n",
      "   -------------- ------------------------- 10.2/28.0 MB 518.3 kB/s eta 0:00:35\n",
      "   -------------- ------------------------- 10.5/28.0 MB 514.8 kB/s eta 0:00:34\n",
      "   -------------- ------------------------- 10.5/28.0 MB 514.8 kB/s eta 0:00:34\n",
      "   --------------- ------------------------ 10.7/28.0 MB 514.6 kB/s eta 0:00:34\n",
      "   --------------- ------------------------ 10.7/28.0 MB 514.6 kB/s eta 0:00:34\n",
      "   --------------- ------------------------ 11.0/28.0 MB 514.9 kB/s eta 0:00:33\n",
      "   --------------- ------------------------ 11.0/28.0 MB 514.9 kB/s eta 0:00:33\n",
      "   ---------------- ----------------------- 11.3/28.0 MB 515.1 kB/s eta 0:00:33\n",
      "   ---------------- ----------------------- 11.3/28.0 MB 515.1 kB/s eta 0:00:33\n",
      "   ---------------- ----------------------- 11.3/28.0 MB 515.1 kB/s eta 0:00:33\n",
      "   ---------------- ----------------------- 11.5/28.0 MB 516.0 kB/s eta 0:00:32\n",
      "   ---------------- ----------------------- 11.5/28.0 MB 516.0 kB/s eta 0:00:32\n",
      "   ---------------- ----------------------- 11.5/28.0 MB 516.0 kB/s eta 0:00:32\n",
      "   ---------------- ----------------------- 11.8/28.0 MB 511.6 kB/s eta 0:00:32\n",
      "   ---------------- ----------------------- 11.8/28.0 MB 511.6 kB/s eta 0:00:32\n",
      "   ---------------- ----------------------- 11.8/28.0 MB 511.6 kB/s eta 0:00:32\n",
      "   ----------------- ---------------------- 12.1/28.0 MB 506.4 kB/s eta 0:00:32\n",
      "   ----------------- ---------------------- 12.1/28.0 MB 506.4 kB/s eta 0:00:32\n",
      "   ----------------- ---------------------- 12.1/28.0 MB 506.4 kB/s eta 0:00:32\n",
      "   ----------------- ---------------------- 12.3/28.0 MB 502.8 kB/s eta 0:00:32\n",
      "   ----------------- ---------------------- 12.3/28.0 MB 502.8 kB/s eta 0:00:32\n",
      "   ----------------- ---------------------- 12.3/28.0 MB 502.8 kB/s eta 0:00:32\n",
      "   ----------------- ---------------------- 12.6/28.0 MB 497.2 kB/s eta 0:00:31\n",
      "   ----------------- ---------------------- 12.6/28.0 MB 497.2 kB/s eta 0:00:31\n",
      "   ----------------- ---------------------- 12.6/28.0 MB 497.2 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 12.8/28.0 MB 492.8 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 12.8/28.0 MB 492.8 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 12.8/28.0 MB 492.8 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 12.8/28.0 MB 492.8 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 13.1/28.0 MB 487.6 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 13.1/28.0 MB 487.6 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 13.1/28.0 MB 487.6 kB/s eta 0:00:31\n",
      "   ------------------- -------------------- 13.4/28.0 MB 483.8 kB/s eta 0:00:31\n",
      "   ------------------- -------------------- 13.4/28.0 MB 483.8 kB/s eta 0:00:31\n",
      "   ------------------- -------------------- 13.4/28.0 MB 483.8 kB/s eta 0:00:31\n",
      "   ------------------- -------------------- 13.4/28.0 MB 483.8 kB/s eta 0:00:31\n",
      "   ------------------- -------------------- 13.6/28.0 MB 479.3 kB/s eta 0:00:30\n",
      "   ------------------- -------------------- 13.6/28.0 MB 479.3 kB/s eta 0:00:30\n",
      "   ------------------- -------------------- 13.6/28.0 MB 479.3 kB/s eta 0:00:30\n",
      "   ------------------- -------------------- 13.6/28.0 MB 479.3 kB/s eta 0:00:30\n",
      "   ------------------- -------------------- 13.9/28.0 MB 473.4 kB/s eta 0:00:30\n",
      "   ------------------- -------------------- 13.9/28.0 MB 473.4 kB/s eta 0:00:30\n",
      "   ------------------- -------------------- 13.9/28.0 MB 473.4 kB/s eta 0:00:30\n",
      "   -------------------- ------------------- 14.2/28.0 MB 470.7 kB/s eta 0:00:30\n",
      "   -------------------- ------------------- 14.2/28.0 MB 470.7 kB/s eta 0:00:30\n",
      "   -------------------- ------------------- 14.2/28.0 MB 470.7 kB/s eta 0:00:30\n",
      "   -------------------- ------------------- 14.4/28.0 MB 446.6 kB/s eta 0:00:31\n",
      "   -------------------- ------------------- 14.4/28.0 MB 446.6 kB/s eta 0:00:31\n",
      "   -------------------- ------------------- 14.4/28.0 MB 446.6 kB/s eta 0:00:31\n",
      "   -------------------- ------------------- 14.4/28.0 MB 446.6 kB/s eta 0:00:31\n",
      "   -------------------- ------------------- 14.7/28.0 MB 413.5 kB/s eta 0:00:33\n",
      "   -------------------- ------------------- 14.7/28.0 MB 413.5 kB/s eta 0:00:33\n",
      "   -------------------- ------------------- 14.7/28.0 MB 413.5 kB/s eta 0:00:33\n",
      "   -------------------- ------------------- 14.7/28.0 MB 413.5 kB/s eta 0:00:33\n",
      "   -------------------- ------------------- 14.7/28.0 MB 413.5 kB/s eta 0:00:33\n",
      "   --------------------- ------------------ 14.9/28.0 MB 388.1 kB/s eta 0:00:34\n",
      "   --------------------- ------------------ 14.9/28.0 MB 388.1 kB/s eta 0:00:34\n",
      "   --------------------- ------------------ 14.9/28.0 MB 388.1 kB/s eta 0:00:34\n",
      "   --------------------- ------------------ 14.9/28.0 MB 388.1 kB/s eta 0:00:34\n",
      "   --------------------- ------------------ 15.2/28.0 MB 380.9 kB/s eta 0:00:34\n",
      "   --------------------- ------------------ 15.2/28.0 MB 380.9 kB/s eta 0:00:34\n",
      "   --------------------- ------------------ 15.2/28.0 MB 380.9 kB/s eta 0:00:34\n",
      "   --------------------- ------------------ 15.2/28.0 MB 380.9 kB/s eta 0:00:34\n",
      "   --------------------- ------------------ 15.2/28.0 MB 380.9 kB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 15.5/28.0 MB 370.7 kB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 15.5/28.0 MB 370.7 kB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 15.5/28.0 MB 370.7 kB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 15.5/28.0 MB 370.7 kB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 15.7/28.0 MB 362.0 kB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 15.7/28.0 MB 362.0 kB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 15.7/28.0 MB 362.0 kB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 15.7/28.0 MB 362.0 kB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 16.0/28.0 MB 360.3 kB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 16.0/28.0 MB 360.3 kB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 16.0/28.0 MB 360.3 kB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 16.3/28.0 MB 360.0 kB/s eta 0:00:33\n",
      "   ----------------------- ---------------- 16.3/28.0 MB 360.0 kB/s eta 0:00:33\n",
      "   ----------------------- ---------------- 16.3/28.0 MB 360.0 kB/s eta 0:00:33\n",
      "   ----------------------- ---------------- 16.5/28.0 MB 358.5 kB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 16.5/28.0 MB 358.5 kB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 16.5/28.0 MB 358.5 kB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 16.8/28.0 MB 355.1 kB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 16.8/28.0 MB 355.1 kB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 16.8/28.0 MB 355.1 kB/s eta 0:00:32\n",
      "   ------------------------ --------------- 17.0/28.0 MB 355.4 kB/s eta 0:00:31\n",
      "   ------------------------ --------------- 17.0/28.0 MB 355.4 kB/s eta 0:00:31\n",
      "   ------------------------ --------------- 17.0/28.0 MB 355.4 kB/s eta 0:00:31\n",
      "   ------------------------ --------------- 17.3/28.0 MB 354.5 kB/s eta 0:00:31\n",
      "   ------------------------ --------------- 17.3/28.0 MB 354.5 kB/s eta 0:00:31\n",
      "   ------------------------ --------------- 17.3/28.0 MB 354.5 kB/s eta 0:00:31\n",
      "   ------------------------- -------------- 17.6/28.0 MB 353.8 kB/s eta 0:00:30\n",
      "   ------------------------- -------------- 17.6/28.0 MB 353.8 kB/s eta 0:00:30\n",
      "   ------------------------- -------------- 17.6/28.0 MB 353.8 kB/s eta 0:00:30\n",
      "   ------------------------- -------------- 17.8/28.0 MB 351.5 kB/s eta 0:00:29\n",
      "   ------------------------- -------------- 17.8/28.0 MB 351.5 kB/s eta 0:00:29\n",
      "   ------------------------- -------------- 17.8/28.0 MB 351.5 kB/s eta 0:00:29\n",
      "   ------------------------- -------------- 18.1/28.0 MB 351.7 kB/s eta 0:00:29\n",
      "   ------------------------- -------------- 18.1/28.0 MB 351.7 kB/s eta 0:00:29\n",
      "   ------------------------- -------------- 18.1/28.0 MB 351.7 kB/s eta 0:00:29\n",
      "   -------------------------- ------------- 18.4/28.0 MB 357.7 kB/s eta 0:00:27\n",
      "   -------------------------- ------------- 18.4/28.0 MB 357.7 kB/s eta 0:00:27\n",
      "   -------------------------- ------------- 18.4/28.0 MB 357.7 kB/s eta 0:00:27\n",
      "   -------------------------- ------------- 18.6/28.0 MB 364.1 kB/s eta 0:00:26\n",
      "   -------------------------- ------------- 18.6/28.0 MB 364.1 kB/s eta 0:00:26\n",
      "   -------------------------- ------------- 18.6/28.0 MB 364.1 kB/s eta 0:00:26\n",
      "   -------------------------- ------------- 18.9/28.0 MB 365.7 kB/s eta 0:00:25\n",
      "   -------------------------- ------------- 18.9/28.0 MB 365.7 kB/s eta 0:00:25\n",
      "   -------------------------- ------------- 18.9/28.0 MB 365.7 kB/s eta 0:00:25\n",
      "   --------------------------- ------------ 19.1/28.0 MB 370.6 kB/s eta 0:00:24\n",
      "   --------------------------- ------------ 19.1/28.0 MB 370.6 kB/s eta 0:00:24\n",
      "   --------------------------- ------------ 19.1/28.0 MB 370.6 kB/s eta 0:00:24\n",
      "   --------------------------- ------------ 19.4/28.0 MB 371.1 kB/s eta 0:00:24\n",
      "   --------------------------- ------------ 19.4/28.0 MB 371.1 kB/s eta 0:00:24\n",
      "   --------------------------- ------------ 19.4/28.0 MB 371.1 kB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 19.7/28.0 MB 371.4 kB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 19.7/28.0 MB 371.4 kB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 19.7/28.0 MB 371.4 kB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 19.7/28.0 MB 371.4 kB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 19.7/28.0 MB 371.4 kB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 19.9/28.0 MB 364.9 kB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 19.9/28.0 MB 364.9 kB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 19.9/28.0 MB 364.9 kB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 19.9/28.0 MB 364.9 kB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 19.9/28.0 MB 364.9 kB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 20.2/28.0 MB 354.7 kB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 20.2/28.0 MB 354.7 kB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 20.2/28.0 MB 354.7 kB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 20.2/28.0 MB 354.7 kB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 20.2/28.0 MB 354.7 kB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 20.4/28.0 MB 350.3 kB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 20.4/28.0 MB 350.3 kB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 20.4/28.0 MB 350.3 kB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 20.4/28.0 MB 350.3 kB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 20.4/28.0 MB 350.3 kB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 20.7/28.0 MB 343.8 kB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 20.7/28.0 MB 343.8 kB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 20.7/28.0 MB 343.8 kB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 20.7/28.0 MB 343.8 kB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 20.7/28.0 MB 343.8 kB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 21.0/28.0 MB 334.5 kB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 21.0/28.0 MB 334.5 kB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 21.0/28.0 MB 334.5 kB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 21.0/28.0 MB 334.5 kB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 21.0/28.0 MB 334.5 kB/s eta 0:00:21\n",
      "   ------------------------------ --------- 21.2/28.0 MB 324.2 kB/s eta 0:00:21\n",
      "   ------------------------------ --------- 21.2/28.0 MB 324.2 kB/s eta 0:00:21\n",
      "   ------------------------------ --------- 21.2/28.0 MB 324.2 kB/s eta 0:00:21\n",
      "   ------------------------------ --------- 21.2/28.0 MB 324.2 kB/s eta 0:00:21\n",
      "   ------------------------------ --------- 21.2/28.0 MB 324.2 kB/s eta 0:00:21\n",
      "   ------------------------------ --------- 21.5/28.0 MB 319.1 kB/s eta 0:00:21\n",
      "   ------------------------------ --------- 21.5/28.0 MB 319.1 kB/s eta 0:00:21\n",
      "   ------------------------------ --------- 21.5/28.0 MB 319.1 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 21.8/28.0 MB 316.9 kB/s eta 0:00:20\n",
      "   ------------------------------- -------- 21.8/28.0 MB 316.9 kB/s eta 0:00:20\n",
      "   ------------------------------- -------- 21.8/28.0 MB 316.9 kB/s eta 0:00:20\n",
      "   ------------------------------- -------- 21.8/28.0 MB 316.9 kB/s eta 0:00:20\n",
      "   ------------------------------- -------- 21.8/28.0 MB 316.9 kB/s eta 0:00:20\n",
      "   ------------------------------- -------- 22.0/28.0 MB 312.0 kB/s eta 0:00:20\n",
      "   ------------------------------- -------- 22.0/28.0 MB 312.0 kB/s eta 0:00:20\n",
      "   ------------------------------- -------- 22.0/28.0 MB 312.0 kB/s eta 0:00:20\n",
      "   ------------------------------- -------- 22.0/28.0 MB 312.0 kB/s eta 0:00:20\n",
      "   ------------------------------- -------- 22.0/28.0 MB 312.0 kB/s eta 0:00:20\n",
      "   ------------------------------- -------- 22.0/28.0 MB 312.0 kB/s eta 0:00:20\n",
      "   ------------------------------- -------- 22.3/28.0 MB 306.3 kB/s eta 0:00:19\n",
      "   ------------------------------- -------- 22.3/28.0 MB 306.3 kB/s eta 0:00:19\n",
      "   ------------------------------- -------- 22.3/28.0 MB 306.3 kB/s eta 0:00:19\n",
      "   ------------------------------- -------- 22.3/28.0 MB 306.3 kB/s eta 0:00:19\n",
      "   ------------------------------- -------- 22.3/28.0 MB 306.3 kB/s eta 0:00:19\n",
      "   -------------------------------- ------- 22.5/28.0 MB 303.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 22.5/28.0 MB 303.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 22.5/28.0 MB 303.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 22.5/28.0 MB 303.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 22.8/28.0 MB 303.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 22.8/28.0 MB 303.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 22.8/28.0 MB 303.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 23.1/28.0 MB 302.3 kB/s eta 0:00:17\n",
      "   -------------------------------- ------- 23.1/28.0 MB 302.3 kB/s eta 0:00:17\n",
      "   -------------------------------- ------- 23.1/28.0 MB 302.3 kB/s eta 0:00:17\n",
      "   -------------------------------- ------- 23.1/28.0 MB 302.3 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 23.3/28.0 MB 302.4 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 23.3/28.0 MB 302.4 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 23.3/28.0 MB 302.4 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 23.3/28.0 MB 302.4 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 23.6/28.0 MB 302.8 kB/s eta 0:00:15\n",
      "   --------------------------------- ------ 23.6/28.0 MB 302.8 kB/s eta 0:00:15\n",
      "   --------------------------------- ------ 23.6/28.0 MB 302.8 kB/s eta 0:00:15\n",
      "   --------------------------------- ------ 23.6/28.0 MB 302.8 kB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 23.9/28.0 MB 301.7 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 23.9/28.0 MB 301.7 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 23.9/28.0 MB 301.7 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 23.9/28.0 MB 301.7 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 23.9/28.0 MB 301.7 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 24.1/28.0 MB 299.4 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 24.1/28.0 MB 299.4 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 24.1/28.0 MB 299.4 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 24.1/28.0 MB 299.4 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 24.1/28.0 MB 299.4 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 24.1/28.0 MB 299.4 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 24.4/28.0 MB 297.3 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 24.4/28.0 MB 297.3 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 24.4/28.0 MB 297.3 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 24.4/28.0 MB 297.3 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 24.4/28.0 MB 297.3 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 24.4/28.0 MB 297.3 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 24.4/28.0 MB 297.3 kB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 24.6/28.0 MB 290.0 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.6/28.0 MB 290.0 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.6/28.0 MB 290.0 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.6/28.0 MB 290.0 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.6/28.0 MB 290.0 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.6/28.0 MB 290.0 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.6/28.0 MB 290.0 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.6/28.0 MB 290.0 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.6/28.0 MB 290.0 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.9/28.0 MB 271.5 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.9/28.0 MB 271.5 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.9/28.0 MB 271.5 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.9/28.0 MB 271.5 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.9/28.0 MB 271.5 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.9/28.0 MB 271.5 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 24.9/28.0 MB 271.5 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 25.2/28.0 MB 258.3 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 25.2/28.0 MB 258.3 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 25.2/28.0 MB 258.3 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 25.2/28.0 MB 258.3 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 25.2/28.0 MB 258.3 kB/s eta 0:00:11\n",
      "   ------------------------------------ --- 25.4/28.0 MB 255.0 kB/s eta 0:00:10\n",
      "   ------------------------------------ --- 25.4/28.0 MB 255.0 kB/s eta 0:00:10\n",
      "   ------------------------------------ --- 25.4/28.0 MB 255.0 kB/s eta 0:00:10\n",
      "   ------------------------------------ --- 25.4/28.0 MB 255.0 kB/s eta 0:00:10\n",
      "   ------------------------------------ --- 25.7/28.0 MB 248.8 kB/s eta 0:00:10\n",
      "   ------------------------------------ --- 25.7/28.0 MB 248.8 kB/s eta 0:00:10\n",
      "   ------------------------------------ --- 25.7/28.0 MB 248.8 kB/s eta 0:00:10\n",
      "   ------------------------------------ --- 25.7/28.0 MB 248.8 kB/s eta 0:00:10\n",
      "   ------------------------------------- -- 26.0/28.0 MB 248.3 kB/s eta 0:00:09\n",
      "   ------------------------------------- -- 26.0/28.0 MB 248.3 kB/s eta 0:00:09\n",
      "   ------------------------------------- -- 26.0/28.0 MB 248.3 kB/s eta 0:00:09\n",
      "   ------------------------------------- -- 26.2/28.0 MB 246.5 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 26.2/28.0 MB 246.5 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 26.2/28.0 MB 246.5 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 26.2/28.0 MB 246.5 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 26.2/28.0 MB 246.5 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 26.5/28.0 MB 240.9 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 26.5/28.0 MB 240.9 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 26.5/28.0 MB 240.9 kB/s eta 0:00:07\n",
      "   -------------------------------------- - 26.7/28.0 MB 240.7 kB/s eta 0:00:06\n",
      "   -------------------------------------- - 26.7/28.0 MB 240.7 kB/s eta 0:00:06\n",
      "   -------------------------------------- - 26.7/28.0 MB 240.7 kB/s eta 0:00:06\n",
      "   -------------------------------------- - 27.0/28.0 MB 243.9 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 27.0/28.0 MB 243.9 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 27.0/28.0 MB 243.9 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 27.3/28.0 MB 247.8 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 27.3/28.0 MB 247.8 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 27.3/28.0 MB 247.8 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 27.3/28.0 MB 247.8 kB/s eta 0:00:03\n",
      "   ---------------------------------------  27.5/28.0 MB 251.5 kB/s eta 0:00:02\n",
      "   ---------------------------------------  27.5/28.0 MB 251.5 kB/s eta 0:00:02\n",
      "   ---------------------------------------  27.5/28.0 MB 251.5 kB/s eta 0:00:02\n",
      "   ---------------------------------------  27.5/28.0 MB 251.5 kB/s eta 0:00:02\n",
      "   ---------------------------------------  27.5/28.0 MB 251.5 kB/s eta 0:00:02\n",
      "   ---------------------------------------  27.8/28.0 MB 252.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  27.8/28.0 MB 252.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  27.8/28.0 MB 252.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  27.8/28.0 MB 252.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.0/28.0 MB 251.3 kB/s eta 0:00:00\n",
      "Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl (31 kB)\n",
      "Installing collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 16.1.0\n",
      "    Uninstalling pyarrow-16.1.0:\n",
      "      Successfully uninstalled pyarrow-16.1.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.8\n",
      "    Uninstalling dill-0.3.8:\n",
      "      Successfully uninstalled dill-0.3.8\n",
      "Successfully installed datasets-4.4.1 dill-0.4.0 multiprocess-0.70.18 pyarrow-22.0.0 xxhash-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\anaconda\\Lib\\site-packages\\~yarrow'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cffc7ae1-f49d-4856-94cb-89591890bb47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in d:\\anaconda\\lib\\site-packages (24.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 130.0 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 130.0 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 130.0 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 130.0 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 130.0 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 130.0 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 130.0 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 130.0 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 130.0 kB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 134.7 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 134.7 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 134.7 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 134.7 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 134.7 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 134.7 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 1.0/1.8 MB 147.6 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 1.0/1.8 MB 147.6 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 1.0/1.8 MB 147.6 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 1.0/1.8 MB 147.6 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 1.0/1.8 MB 147.6 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 1.0/1.8 MB 147.6 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 1.0/1.8 MB 147.6 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 1.0/1.8 MB 147.6 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 1.0/1.8 MB 147.6 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 146.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 146.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 146.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 146.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 146.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 146.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 146.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 146.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 146.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 146.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 146.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 146.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 146.2 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 130.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 130.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 130.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 130.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 130.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 130.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 130.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 130.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 130.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 130.9 kB/s eta 0:00:02\n",
      "   ---------------------------------------- 1.8/1.8 MB 123.8 kB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-25.3\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38bd07d6-5156-445a-a527-abe388186d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in d:\\anaconda\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in d:\\anaconda\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in d:\\anaconda\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\anaconda\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\anaconda\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\anaconda\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc50869-27af-4289-871b-327c4e49da62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\anaconda\\lib\\site-packages (4.57.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in d:\\anaconda\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\anaconda\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\anaconda\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers pandas scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5768a7dc-9fa5-4bde-8571-58d5a355ffe7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'httpx' has no attribute 'RequestError'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_scheduler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearLR\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset  \u001b[38;5;66;03m# Reliable HF loader\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# ------------------- 1. Load your original data -------------------\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.4.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Column, Dataset\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:77\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_files\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\arrow_reader.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _split_re, filenames_for_dataset_split\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryTable, MemoryMappedTable, Table, concat_tables\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\download\\__init__.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloadConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloadManager\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloadMode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreamingDownloadManager\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m ]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadManager, DownloadMode\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingDownloadManager\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\download\\download_manager.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;28;01mas\u001b[39;00m hf_tqdm\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     ArchiveIterable,\n\u001b[0;32m     34\u001b[0m     FilesIterable,\n\u001b[0;32m     35\u001b[0m     cached_path,\n\u001b[0;32m     36\u001b[0m     is_relative_path,\n\u001b[0;32m     37\u001b[0m     stack_multiprocessing_download_progress_bars,\n\u001b[0;32m     38\u001b[0m     url_or_path_join,\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_size_checksum_dict\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger, tqdm\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\utils\\file_utils.py:69\u001b[0m\n\u001b[0;32m     60\u001b[0m INCOMPLETE_SUFFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m, Path)\n\u001b[0;32m     64\u001b[0m CONNECTION_ERRORS_TO_RETRY \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     65\u001b[0m     _AiohttpClientError,\n\u001b[0;32m     66\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mTimeoutError,\n\u001b[0;32m     67\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[0;32m     68\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout,\n\u001b[1;32m---> 69\u001b[0m     httpx\u001b[38;5;241m.\u001b[39mRequestError,\n\u001b[0;32m     70\u001b[0m )\n\u001b[0;32m     71\u001b[0m SERVER_UNAVAILABLE_CODE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m504\u001b[39m\n\u001b[0;32m     72\u001b[0m RATE_LIMIT_CODE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m429\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'httpx' has no attribute 'RequestError'"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Install (run once in Anaconda Prompt/Jupyter)\n",
    "# --------------------------------------------------------------\n",
    "# pip install datasets  # For HF datasets\n",
    "# conda install pyarrow -c conda-forge  # If needed\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  # or cpuonly\n",
    "# pip install transformers pandas scikit-learn tqdm\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# COMPLETE WORKING CODE – copy-paste and run\n",
    "# --------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset  # Reliable HF loader\n",
    "import os\n",
    "\n",
    "# ------------------- 1. Load your original data -------------------\n",
    "print(\"Loading your cefr_augmented.csv...\")\n",
    "df_user = pd.read_csv(\"cefr_augmented.csv\", encoding='latin1')\n",
    "df_user['text'] = df_user['text'].str.replace('Ã¼', 'ü').str.replace('Ã¤', 'ä')\\\n",
    "                                 .str.replace('Ã¶', 'ö').str.replace('ÃŸ', 'ß')\n",
    "df_user['CEFR'] = df_user['CEFR'].replace({'C2': 'C1'})\n",
    "\n",
    "# ------------------- 2. Load public datasets via HF library -------------------\n",
    "print(\"Loading public German CEFR datasets...\")\n",
    "\n",
    "# Dataset 1: UniversalCEFR/elg_cefr_de (~60 samples)\n",
    "ds1 = load_dataset(\"UniversalCEFR/elg_cefr_de\")\n",
    "df1 = ds1['train'].to_pandas() if 'train' in ds1 else ds1.to_pandas()\n",
    "df1 = df1[['text', 'cefr_level']].rename(columns={'cefr_level': 'CEFR'})\n",
    "df1['CEFR'] = df1['CEFR'].replace({'C2': 'C1'})\n",
    "\n",
    "# Dataset 2: EliasAhl/german-cefr (~500 samples)\n",
    "ds2 = load_dataset(\"EliasAhl/german-cefr\")\n",
    "df2 = ds2['train'].to_pandas() if 'train' in ds2 else ds2.to_pandas()\n",
    "df2 = df2[['text', 'cefrLevel']].rename(columns={'cefrLevel': 'CEFR'})\n",
    "df2['CEFR'] = df2['CEFR'].replace({'C2': 'C1'})\n",
    "\n",
    "# ------------------- 3. Merge everything -------------------\n",
    "df_all = pd.concat([df_user[['text', 'CEFR']], df1, df2], ignore_index=True)\n",
    "df_all.drop_duplicates(subset='text', inplace=True)\n",
    "df_all.dropna(subset=['text', 'CEFR'], inplace=True)\n",
    "\n",
    "df_all.to_csv(\"cefr_final_merged.csv\", index=False)\n",
    "print(f\"\\nSUCCESS! Merged dataset created: {len(df_all)} sentences\")\n",
    "print(\"CEFR distribution:\")\n",
    "print(df_all['CEFR'].value_counts().sort_index())\n",
    "\n",
    "# ------------------- 4. Prepare labels & split -------------------\n",
    "labels = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "df_all['label'] = df_all['CEFR'].map(label2id)\n",
    "\n",
    "train_df, test_df = train_test_split(df_all, test_size=0.2, random_state=42, stratify=df_all['label'])\n",
    "\n",
    "# ------------------- 5. Tokenizer + Dataset -------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/gelectra-base\")\n",
    "\n",
    "class CEFRDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "        self.labels = labels.tolist()\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self): return len(self.labels)\n",
    "\n",
    "train_dataset = CEFRDataset(train_df['text'], train_df['label'])\n",
    "test_dataset  = CEFRDataset(test_df['text'], test_df['label'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# ------------------- 6. Model -------------------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"deepset/gelectra-base\", num_labels=5, id2label={i: l for i, l in enumerate(labels)}, label2id=label2id\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "num_epochs = 4\n",
    "scheduler = LinearLR(optimizer, total_iters=num_epochs * len(train_loader))\n",
    "\n",
    "# ------------------- 7. Training -------------------\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} – avg loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ------------------- 8. Evaluation -------------------\n",
    "model.eval()\n",
    "preds, trues = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        preds.extend(torch.argmax(outputs.logits, dim=-1).cpu().numpy())\n",
    "        trues.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "print(f\"\\nFINAL TEST ACCURACY: {accuracy_score(trues, preds):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(trues, preds, target_names=labels))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(trues, preds))\n",
    "\n",
    "# ------------------- 9. Save model -------------------\n",
    "os.makedirs(\"cefr_german_model\", exist_ok=True)\n",
    "model.save_pretrained(\"cefr_german_model\")\n",
    "tokenizer.save_pretrained(\"cefr_german_model\")\n",
    "print(\"Model saved → ./cefr_german_model\")\n",
    "\n",
    "# ------------------- 10. Predict function -------------------\n",
    "def predict_cefr(sentence):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    return labels[torch.argmax(logits, dim=-1).item()]\n",
    "\n",
    "# Test\n",
    "print(\"\\nExamples:\")\n",
    "print(predict_cefr(\"Hallo, wie geht es dir?\"))                                 # → A1/A2\n",
    "print(predict_cefr(\"Die Auswirkungen des Klimawandels sind komplex und global.\"))  # → C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "551c31bb-2b4a-4f34-995d-eed223e61a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting httpx==0.27.0\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting httpcore==1.0.5\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: anyio in d:\\anaconda\\lib\\site-packages (from httpx==0.27.0) (4.2.0)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\lib\\site-packages (from httpx==0.27.0) (2025.10.5)\n",
      "Requirement already satisfied: idna in d:\\anaconda\\lib\\site-packages (from httpx==0.27.0) (2.10)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda\\lib\\site-packages (from httpx==0.27.0) (1.3.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.0.5)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: h11, httpcore, httpx\n",
      "\n",
      "  Attempting uninstall: h11\n",
      "\n",
      "    Found existing installation: h11 0.9.0\n",
      "\n",
      "    Uninstalling h11-0.9.0:\n",
      "\n",
      "      Successfully uninstalled h11-0.9.0\n",
      "\n",
      "   ---------------------------------------- 0/3 [h11]\n",
      "   ---------------------------------------- 0/3 [h11]\n",
      "   ---------------------------------------- 0/3 [h11]\n",
      "  Attempting uninstall: httpcore\n",
      "   ---------------------------------------- 0/3 [h11]\n",
      "    Found existing installation: httpcore 0.9.1\n",
      "   ---------------------------------------- 0/3 [h11]\n",
      "   ------------- -------------------------- 1/3 [httpcore]\n",
      "    Uninstalling httpcore-0.9.1:\n",
      "   ------------- -------------------------- 1/3 [httpcore]\n",
      "      Successfully uninstalled httpcore-0.9.1\n",
      "   ------------- -------------------------- 1/3 [httpcore]\n",
      "   ------------- -------------------------- 1/3 [httpcore]\n",
      "   ------------- -------------------------- 1/3 [httpcore]\n",
      "   ------------- -------------------------- 1/3 [httpcore]\n",
      "  Attempting uninstall: httpx\n",
      "   ------------- -------------------------- 1/3 [httpcore]\n",
      "    Found existing installation: httpx 0.13.3\n",
      "   ------------- -------------------------- 1/3 [httpcore]\n",
      "    Uninstalling httpx-0.13.3:\n",
      "   ------------- -------------------------- 1/3 [httpcore]\n",
      "      Successfully uninstalled httpx-0.13.3\n",
      "   ------------- -------------------------- 1/3 [httpcore]\n",
      "   -------------------------- ------------- 2/3 [httpx]\n",
      "   -------------------------- ------------- 2/3 [httpx]\n",
      "   -------------------------- ------------- 2/3 [httpx]\n",
      "   -------------------------- ------------- 2/3 [httpx]\n",
      "   -------------------------- ------------- 2/3 [httpx]\n",
      "   -------------------------- ------------- 2/3 [httpx]\n",
      "   -------------------------- ------------- 2/3 [httpx]\n",
      "   ---------------------------------------- 3/3 [httpx]\n",
      "\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "googletrans 4.0.0rc1 requires httpx==0.13.3, but you have httpx 0.27.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install httpx==0.27.0 httpcore==1.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01b01847-d5e9-4d17-b1cc-c49ef0ed6715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\anaconda\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in d:\\anaconda\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in d:\\anaconda\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\anaconda\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in d:\\anaconda\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\anaconda\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in d:\\anaconda\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in d:\\anaconda\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in d:\\anaconda\\lib\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\anaconda\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: anyio in d:\\anaconda\\lib\\site-packages (from httpx<1.0.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in d:\\anaconda\\lib\\site-packages (from httpx<1.0.0->datasets) (2.10)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda\\lib\\site-packages (from httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anaconda\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7bec122-8d23-4531-a66e-58901fd40a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Ready! Total 2273 sentences\n",
      "Training model (takes 20-40 seconds)...\n",
      "\n",
      "FINAL TEST ACCURACY: 0.7363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.92      0.47      0.62        51\n",
      "          A2       0.72      0.81      0.76       102\n",
      "          B1       0.75      0.64      0.69       105\n",
      "          B2       0.76      0.73      0.74       102\n",
      "          C1       0.69      0.92      0.79        95\n",
      "\n",
      "    accuracy                           0.74       455\n",
      "   macro avg       0.77      0.71      0.72       455\n",
      "weighted avg       0.75      0.74      0.73       455\n",
      "\n",
      "\n",
      "Model saved to ./cefr_german_model\n",
      "To predict later, run this in a new cell:\n",
      "import joblib\n",
      "vectorizer = joblib.load(\"cefr_german_model/vectorizer.pkl\")\n",
      "model = joblib.load(\"cefr_german_model/model.pkl\")\n",
      "labels = joblib.load(\"cefr_german_model/labels.pkl\")\n",
      "def predict(sentence):\n",
      "    vec = vectorizer.transform([sentence])\n",
      "    p = model.predict(vec)[0]\n",
      "    return labels[p]\n",
      "print(predict(\"Hallo, wie geht es dir?\"))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df_user = pd.read_csv(\"cefr_augmented.csv\", encoding='latin1')\n",
    "df_user['text'] = df_user['text'].str.replace('Ã¼', 'ü').str.replace('Ã¤', 'ä')\\\n",
    "                                 .str.replace('Ã¶', 'ö').str.replace('ÃŸ', 'ß')\n",
    "df_user['CEFR'] = df_user['CEFR'].replace({'C2': 'C1'})\n",
    "\n",
    "from datasets import load_dataset\n",
    "df1 = load_dataset(\"UniversalCEFR/elg_cefr_de\", split=\"train\").to_pandas()[['text','cefr_level']].rename(columns={'cefr_level':'CEFR'})\n",
    "df2 = load_dataset(\"EliasAhl/german-cefr\", split=\"train\").to_pandas()[['text','cefrLevel']].rename(columns={'cefrLevel':'CEFR'})\n",
    "\n",
    "for df in [df1, df2]:\n",
    "    df['CEFR'] = df['CEFR'].replace({'C2': 'C1'})\n",
    "\n",
    "df = pd.concat([df_user[['text','CEFR']], df1, df2]).drop_duplicates('text').dropna()\n",
    "df.to_csv(\"cefr_final_merged.csv\", index=False)\n",
    "print(f\"Ready! Total {len(df)} sentences\")\n",
    "\n",
    "labels = ['A1','A2','B1','B2','C1']\n",
    "df['label'] = df['CEFR'].map({l:i for i,l in enumerate(labels)})\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,3), lowercase=False)\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "print(\"Training model (takes 20-40 seconds)...\")\n",
    "model = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(f\"\\nFINAL TEST ACCURACY: {acc:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=labels))\n",
    "\n",
    "os.makedirs(\"cefr_german_model\", exist_ok=True)\n",
    "import joblib\n",
    "joblib.dump(model, \"cefr_german_model/model.pkl\")\n",
    "joblib.dump(vectorizer, \"cefr_german_model/vectorizer.pkl\")\n",
    "joblib.dump(labels, \"cefr_german_model/labels.pkl\")\n",
    "print(\"\\nModel saved to ./cefr_german_model\")\n",
    "print(\"To predict later, run this in a new cell:\")\n",
    "print('''import joblib\n",
    "vectorizer = joblib.load(\"cefr_german_model/vectorizer.pkl\")\n",
    "model = joblib.load(\"cefr_german_model/model.pkl\")\n",
    "labels = joblib.load(\"cefr_german_model/labels.pkl\")\n",
    "def predict(sentence):\n",
    "    vec = vectorizer.transform([sentence])\n",
    "    p = model.predict(vec)[0]\n",
    "    return labels[p]\n",
    "print(predict(\"Hallo, wie geht es dir?\"))''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0f10a2a-c700-4d0b-97d3-fc899fb24474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready: 2273 sentences\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import joblib\n",
    "\n",
    "# Load your data + public data\n",
    "df_user = pd.read_csv(\"cefr_augmented.csv\", encoding='latin1')\n",
    "df_user['text'] = df_user['text'].str.replace('Ã¼', 'ü').str.replace('Ã¤', 'ä')\\\n",
    "                                 .str.replace('Ã¶', 'ö').str.replace('ÃŸ', 'ß')\n",
    "df_user['CEFR'] = df_user['CEFR'].replace({'C2': 'C1'})\n",
    "\n",
    "df1 = load_dataset(\"UniversalCEFR/elg_cefr_de\", split=\"train\").to_pandas()[['text','cefr_level']].rename(columns={'cefr_level':'CEFR'})\n",
    "df2 = load_dataset(\"EliasAhl/german-cefr\", split=\"train\").to_pandas()[['text','cefrLevel']].rename(columns={'cefrLevel':'CEFR'})\n",
    "\n",
    "for df in [df1, df2]: \n",
    "    df['CEFR'] = df['CEFR'].replace({'C2': 'C1'})\n",
    "\n",
    "df = pd.concat([df_user[['text','CEFR']], df1, df2]).drop_duplicates('text').dropna()\n",
    "\n",
    "labels = ['A1','A2','B1','B2','C1']\n",
    "df['label'] = df['CEFR'].map({l:i for i,l in enumerate(labels)})\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "print(\"Data ready:\", len(df), \"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a747c2c-7a42-4616-89f9-265e93c52e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LinearSVC...\n",
      "\n",
      "FINAL ACCURACY: 0.7912\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.85      0.80      0.83        51\n",
      "          A2       0.77      0.83      0.80       102\n",
      "          B1       0.74      0.67      0.70       105\n",
      "          B2       0.76      0.75      0.75       102\n",
      "          C1       0.85      0.93      0.89        95\n",
      "\n",
      "    accuracy                           0.79       455\n",
      "   macro avg       0.80      0.80      0.80       455\n",
      "weighted avg       0.79      0.79      0.79       455\n",
      "\n",
      "Perfect model saved!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# This vectorizer is the one that actually works for CEFR\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    ngram_range=(3,5),      # 3–5 character n-grams = gold for CEFR\n",
    "    lowercase=False,\n",
    "    max_features=None       # use all features\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test  = vectorizer.transform(test_df['text'])\n",
    "y_train = train_df['label']\n",
    "y_test  = test_df['label']\n",
    "\n",
    "print(\"Training LinearSVC...\")\n",
    "model = LinearSVC(C=1.0, class_weight='balanced', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "print(f\"\\nFINAL ACCURACY: {accuracy_score(y_test, pred):.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=labels))\n",
    "\n",
    "# Save the working model\n",
    "import os\n",
    "os.makedirs(\"cefr_german_model\", exist_ok=True)\n",
    "joblib.dump(model, \"cefr_german_model/model.pkl\")\n",
    "joblib.dump(vectorizer, \"cefr_german_model/vectorizer.pkl\")\n",
    "joblib.dump(labels, \"cefr_german_model/labels.pkl\")\n",
    "print(\"Perfect model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a5d3aba9-1832-4958-ac48-257754204fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a German sentence:  Ich denke, dass wir mehr für den Umweltschutz tun sollten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CEFR Level: C1\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load your perfect 79.1% model\n",
    "vectorizer = joblib.load(\"cefr_german_model/vectorizer.pkl\")\n",
    "model      = joblib.load(\"cefr_german_model/model.pkl\")\n",
    "labels     = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "\n",
    "def cefr(sentence):\n",
    "    return labels[model.predict(vectorizer.transform([sentence]))[0]]\n",
    "\n",
    "# One sentence → one prediction → done\n",
    "sentence = input(\"Enter a German sentence: \").strip()\n",
    "\n",
    "if not sentence:\n",
    "    print(\"No input — goodbye!\")\n",
    "else:\n",
    "    level = cefr(sentence)\n",
    "    print(f\"\\nCEFR Level: {level}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60fbfe4d-c422-4c98-90d0-329d2a4bfb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from cefr_final_merged.csv ...\n",
      "Data ready: 2273 sentences loaded from cefr_final_merged.csv\n",
      "CEFR distribution:\n",
      "CEFR\n",
      "A1    254\n",
      "A2    507\n",
      "B1    524\n",
      "B2    511\n",
      "C1    477\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read directly from the merged CSV you already created\n",
    "print(\"Loading data from cefr_final_merged.csv ...\")\n",
    "df = pd.read_csv(\"cefr_final_merged.csv\")\n",
    "\n",
    "# Make sure CEFR column is clean (just in case)\n",
    "df['CEFR'] = df['CEFR'].replace({'C2': 'C1'}).str.strip()\n",
    "\n",
    "# Create numeric labels\n",
    "labels = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "df['label'] = df['CEFR'].map({level: idx for idx, level in enumerate(labels)})\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "\n",
    "print(f\"Data ready: {len(df)} sentences loaded from cefr_final_merged.csv\")\n",
    "print(\"CEFR distribution:\")\n",
    "print(df['CEFR'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "54751e54-dbd2-41d7-a790-3cf98d6303db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading my original data...\n",
      "→ My data: 1259 sentences\n",
      "\n",
      "Step 2: Adding public German CEFR datasets from Hugging Face...\n",
      "→ UniversalCEFR/elg_cefr_de: 509 sentences\n",
      "→ EliasAhl/german-cefr: 606 sentences\n",
      "\n",
      "Step 3: Merging all data and removing duplicates...\n",
      "→ Final dataset: 2273 unique sentences\n",
      "\n",
      "Step 4: Saving as cefr_final_merged.csv\n",
      "cefr_final_merged.csv has been created!\n",
      "You can now open it and see all 2273 sentences with CEFR levels\n",
      "\n",
      "First 10 rows:\n",
      "                                                text CEFR\n",
      "0  M. Meier Müllergasse 1 Stadt X Internationale ...   B2\n",
      "1  Müller Julia Bahnhofsstr. 1 A Stadt X Armenien...   B2\n",
      "2  Michael Meier 1 Zentralplatz 1234. Stadt X Aup...   B2\n",
      "3  Eva Meier Schmidt Müllergasse 12 Stadt X Kroat...   B2\n",
      "4  Abs. Frau EVA SCHMIDT BAHNHOFSTR, , 1234 STADT...   B1\n",
      "5  Maria Schmidt BahnhofsstraÃe - 12 Stadt X . S...   B1\n",
      "6  Stadt X, Internationale Au-pair Vermittlung Ba...   B2\n",
      "7  Meier Katharina . 1234 Stadt X Computer-Spezia...   B2\n",
      "8  Maria Schmidt . BahnhofsstraÃe 12 Stadt X. St...   B2\n",
      "9  Katharina Winkelmann Müllergasse 12 Stadt X Co...   B1\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# HOW I CREATED cefr_final_merged.csv\n",
    "# (Run this cell to show sir — it creates the CSV file)\n",
    "# ======================================================\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Step 1: Loading my original data...\")\n",
    "df_my = pd.read_csv(\"cefr_augmented.csv\", encoding='latin1')\n",
    "\n",
    "# Fix encoding issues\n",
    "df_my['text'] = df_my['text'].str.replace('Ã¼', 'ü').str.replace('Ã¤', 'ä')\\\n",
    "                             .str.replace('Ã¶', 'ö').str.replace('ÃŸ', 'ß')\n",
    "df_my['CEFR'] = df_my['CEFR'].replace({'C2': 'C1'})\n",
    "\n",
    "print(f\"→ My data: {len(df_my)} sentences\")\n",
    "\n",
    "print(\"\\nStep 2: Adding public German CEFR datasets from Hugging Face...\")\n",
    "\n",
    "# Public dataset 1\n",
    "df1 = load_dataset(\"UniversalCEFR/elg_cefr_de\", split=\"train\").to_pandas()\n",
    "df1 = df1[['text', 'cefr_level']].rename(columns={'cefr_level': 'CEFR'})\n",
    "print(f\"→ UniversalCEFR/elg_cefr_de: {len(df1)} sentences\")\n",
    "\n",
    "# Public dataset 2\n",
    "df2 = load_dataset(\"EliasAhl/german-cefr\", split=\"train\").to_pandas()\n",
    "df2 = df2[['text', 'cefrLevel']].rename(columns={'cefrLevel': 'CEFR'})\n",
    "print(f\"→ EliasAhl/german-cefr: {len(df2)} sentences\")\n",
    "\n",
    "# Clean C2 → C1 in both\n",
    "for df in [df1, df2]:\n",
    "    df['CEFR'] = df['CEFR'].replace({'C2': 'C1'})\n",
    "\n",
    "print(\"\\nStep 3: Merging all data and removing duplicates...\")\n",
    "final_df = pd.concat([df_my[['text', 'CEFR']], df1, df2], ignore_index=True)\n",
    "final_df = final_df.drop_duplicates(subset='text').dropna()  # remove empty rows\n",
    "\n",
    "print(f\"→ Final dataset: {len(final_df)} unique sentences\")\n",
    "\n",
    "print(\"\\nStep 4: Saving as cefr_final_merged.csv\")\n",
    "final_df.to_csv(\"cefr_final_merged.csv\", index=False)\n",
    "\n",
    "print(\"cefr_final_merged.csv has been created!\")\n",
    "print(\"You can now open it and see all 2273 sentences with CEFR levels\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(final_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e3e12ed2-1046-410b-b754-93a4d517e6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.7912\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load the merged data you already created (fast, no internet!)\n",
    "df = pd.read_csv(\"cefr_final_merged.csv\")\n",
    "\n",
    "labels = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "df['label'] = df['CEFR'].map({l: i for i, l in enumerate(labels)})\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "# Best model for German CEFR\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), lowercase=False)\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test  = vectorizer.transform(test_df['text'])\n",
    "y_train = train_df['label']\n",
    "y_test  = test_df['label']\n",
    "\n",
    "model = LinearSVC(C=1.0, class_weight='balanced', max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"ACCURACY: {accuracy_score(y_test, model.predict(X_test)):.4f}\")\n",
    "\n",
    "# Save (overwrite old weak model)\n",
    "os.makedirs(\"cefr_german_model\", exist_ok=True)\n",
    "joblib.dump(model,      \"cefr_german_model/model.pkl\")\n",
    "joblib.dump(vectorizer, \"cefr_german_model/vectorizer.pkl\")\n",
    "joblib.dump(labels,     \"cefr_german_model/labels.pkl\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e184b0-a400-4c46-be4a-2a25283b6612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
